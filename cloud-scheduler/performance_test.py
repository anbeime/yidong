#!/usr/bin/env python3
"""
CloudCoder æ€§èƒ½æµ‹è¯•å’Œè´Ÿè½½æµ‹è¯•æ¨¡å—
æµ‹è¯•ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½è¡¨ç°
"""

import asyncio
import concurrent.futures
import json
import time
import psutil
import threading
import requests
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import statistics
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "response_times": [],
            "error_count": 0,
            "success_count": 0,
            "concurrent_users": 0
        }
        self.monitoring = False
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
        self.monitoring = True
        thread = threading.Thread(target=self._monitor_system)
        thread.daemon = True
        thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
    
    def _monitor_system(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        while self.monitoring:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            self.metrics["cpu_usage"].append({
                "timestamp": datetime.now().isoformat(),
                "value": cpu_percent
            })
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            self.metrics["memory_usage"].append({
                "timestamp": datetime.now().isoformat(),
                "value": memory.percent
            })
            
            time.sleep(1)
    
    def record_response_time(self, response_time: float):
        """è®°å½•å“åº”æ—¶é—´"""
        self.metrics["response_times"].append({
            "timestamp": datetime.now().isoformat(),
            "value": response_time
        })
    
    def record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self.metrics["success_count"] += 1
    
    def record_error(self):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        self.metrics["error_count"] += 1
    
    def set_concurrent_users(self, count: int):
        """è®¾ç½®å¹¶å‘ç”¨æˆ·æ•°"""
        self.metrics["concurrent_users"] = count
    
    def get_summary_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦æŠ¥å‘Š"""
        response_times = [rt["value"] for rt in self.metrics["response_times"]]
        cpu_usage = [cpu["value"] for cpu in self.metrics["cpu_usage"]]
        memory_usage = [mem["value"] for mem in self.metrics["memory_usage"]]
        
        total_requests = self.metrics["success_count"] + self.metrics["error_count"]
        success_rate = (self.metrics["success_count"] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "summary": {
                "total_requests": total_requests,
                "successful_requests": self.metrics["success_count"],
                "failed_requests": self.metrics["error_count"],
                "success_rate": f"{success_rate:.2f}%",
                "max_concurrent_users": self.metrics["concurrent_users"]
            },
            "response_time_stats": {
                "min": min(response_times) if response_times else 0,
                "max": max(response_times) if response_times else 0,
                "average": statistics.mean(response_times) if response_times else 0,
                "median": statistics.median(response_times) if response_times else 0,
                "p95": statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else 0,
                "p99": statistics.quantiles(response_times, n=100)[98] if len(response_times) >= 100 else 0
            },
            "system_resource_stats": {
                "cpu_usage": {
                    "min": min(cpu_usage) if cpu_usage else 0,
                    "max": max(cpu_usage) if cpu_usage else 0,
                    "average": statistics.mean(cpu_usage) if cpu_usage else 0
                },
                "memory_usage": {
                    "min": min(memory_usage) if memory_usage else 0,
                    "max": max(memory_usage) if memory_usage else 0,
                    "average": statistics.mean(memory_usage) if memory_usage else 0
                }
            }
        }

class LoadTester:
    """è´Ÿè½½æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8084"):
        self.base_url = base_url
        self.monitor = PerformanceMonitor()
    
    async def single_request_test(self, endpoint: str, method: str = "GET", 
                                 data: Optional[Dict] = None) -> Dict[str, Any]:
        """å•ä¸ªè¯·æ±‚æµ‹è¯•"""
        start_time = time.time()
        
        try:
            if method.upper() == "POST":
                response = requests.post(f"{self.base_url}{endpoint}", 
                                       json=data, timeout=30)
            else:
                response = requests.get(f"{self.base_url}{endpoint}", timeout=30)
            
            response_time = time.time() - start_time
            self.monitor.record_response_time(response_time)
            
            if response.status_code == 200:
                self.monitor.record_success()
                return {
                    "success": True,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "response_size": len(response.content)
                }
            else:
                self.monitor.record_error()
                return {
                    "success": False,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "error": "HTTP Error"
                }
        
        except Exception as e:
            response_time = time.time() - start_time
            self.monitor.record_response_time(response_time)
            self.monitor.record_error()
            return {
                "success": False,
                "response_time": response_time,
                "error": str(e)
            }
    
    async def concurrent_user_test(self, concurrent_users: int, 
                                  requests_per_user: int,
                                  endpoint: str = "/") -> Dict[str, Any]:
        """å¹¶å‘ç”¨æˆ·æµ‹è¯•"""
        logger.info(f"å¼€å§‹å¹¶å‘æµ‹è¯•: {concurrent_users}ä¸ªç”¨æˆ·ï¼Œæ¯ç”¨æˆ·{requests_per_user}ä¸ªè¯·æ±‚")
        
        self.monitor.set_concurrent_users(concurrent_users)
        self.monitor.start_monitoring()
        
        async def user_session():
            """æ¨¡æ‹Ÿå•ä¸ªç”¨æˆ·ä¼šè¯"""
            for _ in range(requests_per_user):
                await self.single_request_test(endpoint)
                await asyncio.sleep(0.1)  # ç”¨æˆ·æ€è€ƒæ—¶é—´
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [user_session() for _ in range(concurrent_users)]
        
        start_time = time.time()
        await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        self.monitor.stop_monitoring()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.monitor.get_summary_report()
        report["test_info"] = {
            "concurrent_users": concurrent_users,
            "requests_per_user": requests_per_user,
            "total_time": total_time,
            "requests_per_second": report["summary"]["total_requests"] / total_time
        }
        
        return report
    
    async def stress_test(self, max_users: int = 100, step: int = 10, 
                         duration_per_step: int = 30) -> Dict[str, Any]:
        """å‹åŠ›æµ‹è¯• - é€æ­¥å¢åŠ è´Ÿè½½"""
        logger.info(f"å¼€å§‹å‹åŠ›æµ‹è¯•: æœ€å¤§{max_users}ç”¨æˆ·ï¼Œæ­¥é•¿{step}ï¼Œæ¯æ­¥{duration_per_step}ç§’")
        
        stress_results = []
        
        for user_count in range(step, max_users + 1, step):
            logger.info(f"æµ‹è¯•è´Ÿè½½: {user_count}ä¸ªå¹¶å‘ç”¨æˆ·")
            
            # è¿è¡Œè´Ÿè½½æµ‹è¯•
            result = await self.concurrent_user_test(
                concurrent_users=user_count,
                requests_per_user=duration_per_step // 2  # æ¯2ç§’ä¸€ä¸ªè¯·æ±‚
            )
            
            stress_results.append({
                "concurrent_users": user_count,
                "response_time_avg": result["response_time_stats"]["average"],
                "success_rate": result["summary"]["success_rate"],
                "cpu_usage_avg": result["system_resource_stats"]["cpu_usage"]["average"],
                "memory_usage_avg": result["system_resource_stats"]["memory_usage"]["average"]
            })
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½ç“¶é¢ˆ
            if float(result["summary"]["success_rate"].replace('%', '')) < 95:
                logger.warning(f"æˆåŠŸç‡ä½äº95%ï¼Œåœæ­¢å‹åŠ›æµ‹è¯•")
                break
            
            # ç­‰å¾…ç³»ç»Ÿæ¢å¤
            await asyncio.sleep(5)
        
        return {
            "stress_test_results": stress_results,
            "performance_bottleneck": self._analyze_bottleneck(stress_results)
        }
    
    def _analyze_bottleneck(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        if not results:
            return {"analysis": "æ— è¶³å¤Ÿæ•°æ®åˆ†æ"}
        
        # å¯»æ‰¾æ€§èƒ½æ‹ç‚¹
        response_times = [r["response_time_avg"] for r in results]
        cpu_usage = [r["cpu_usage_avg"] for r in results]
        memory_usage = [r["memory_usage_avg"] for r in results]
        
        analysis = {
            "max_stable_users": 0,
            "bottleneck_type": "unknown",
            "recommendations": []
        }
        
        # æŸ¥æ‰¾å“åº”æ—¶é—´æ€¥å‰§å¢åŠ çš„ç‚¹
        for i in range(1, len(response_times)):
            if response_times[i] > response_times[i-1] * 2:  # å“åº”æ—¶é—´ç¿»å€
                analysis["max_stable_users"] = results[i-1]["concurrent_users"]
                break
        
        # åˆ†æç“¶é¢ˆç±»å‹
        if max(cpu_usage) > 80:
            analysis["bottleneck_type"] = "CPUå¯†é›†å‹"
            analysis["recommendations"].append("è€ƒè™‘å¢åŠ CPUæ ¸å¿ƒæ•°æˆ–ä¼˜åŒ–ç®—æ³•")
        
        if max(memory_usage) > 80:
            analysis["bottleneck_type"] = "å†…å­˜å¯†é›†å‹"
            analysis["recommendations"].append("è€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        if analysis["bottleneck_type"] == "unknown":
            analysis["bottleneck_type"] = "I/Oæˆ–ç½‘ç»œç“¶é¢ˆ"
            analysis["recommendations"].append("æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± å’Œç½‘ç»œé…ç½®")
        
        return analysis

class CloudCoderPerformanceTester:
    """CloudCoderä¸“ç”¨æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.load_tester = LoadTester()
    
    async def test_code_generation_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ä»£ç ç”ŸæˆåŠŸèƒ½çš„æ€§èƒ½"""
        logger.info("å¼€å§‹æµ‹è¯•ä»£ç ç”Ÿæˆæ€§èƒ½...")
        
        test_data = {
            "requirement": "åˆ›å»ºä¸€ä¸ªç®€å•çš„åšå®¢ç³»ç»Ÿï¼ŒåŒ…å«æ–‡ç« ç®¡ç†å’Œç”¨æˆ·è¯„è®ºåŠŸèƒ½",
            "app_type": "default"
        }
        
        # å•ç”¨æˆ·æ€§èƒ½æµ‹è¯•
        single_result = await self.load_tester.single_request_test(
            "/generate", "POST", test_data
        )
        
        # å¹¶å‘æµ‹è¯•
        concurrent_result = await self.load_tester.concurrent_user_test(
            concurrent_users=5,
            requests_per_user=2,
            endpoint="/generate"
        )
        
        return {
            "single_user_performance": single_result,
            "concurrent_performance": concurrent_result,
            "test_type": "code_generation"
        }
    
    async def test_cloud_api_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç§»åŠ¨äº‘APIè°ƒç”¨æ€§èƒ½"""
        logger.info("å¼€å§‹æµ‹è¯•ç§»åŠ¨äº‘APIæ€§èƒ½...")
        
        test_data = {
            "app_type": "ecommerce",
            "expected_users": 1000
        }
        
        # APIè°ƒç”¨æ€§èƒ½æµ‹è¯•
        result = await self.load_tester.concurrent_user_test(
            concurrent_users=10,
            requests_per_user=3,
            endpoint="/api/cloud/estimate"
        )
        
        return {
            "cloud_api_performance": result,
            "test_type": "cloud_api"
        }
    
    async def run_full_performance_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•å¥—ä»¶...")
        
        results = {
            "test_start_time": datetime.now().isoformat(),
            "test_results": {}
        }
        
        try:
            # 1. åŸºç¡€è´Ÿè½½æµ‹è¯•
            logger.info("1. æ‰§è¡ŒåŸºç¡€è´Ÿè½½æµ‹è¯•...")
            basic_load = await self.load_tester.concurrent_user_test(
                concurrent_users=20,
                requests_per_user=5
            )
            results["test_results"]["basic_load_test"] = basic_load
            
            # 2. ä»£ç ç”Ÿæˆæ€§èƒ½æµ‹è¯•
            logger.info("2. æ‰§è¡Œä»£ç ç”Ÿæˆæ€§èƒ½æµ‹è¯•...")
            code_gen_perf = await self.test_code_generation_performance()
            results["test_results"]["code_generation_performance"] = code_gen_perf
            
            # 3. äº‘APIæ€§èƒ½æµ‹è¯•
            logger.info("3. æ‰§è¡Œäº‘APIæ€§èƒ½æµ‹è¯•...")
            cloud_api_perf = await self.test_cloud_api_performance()
            results["test_results"]["cloud_api_performance"] = cloud_api_perf
            
            # 4. å‹åŠ›æµ‹è¯•
            logger.info("4. æ‰§è¡Œå‹åŠ›æµ‹è¯•...")
            stress_test = await self.load_tester.stress_test(
                max_users=50,
                step=10,
                duration_per_step=20
            )
            results["test_results"]["stress_test"] = stress_test
            
            results["test_end_time"] = datetime.now().isoformat()
            results["test_status"] = "completed"
            
            # ç”Ÿæˆæ€§èƒ½è¯„ä¼°æŠ¥å‘Š
            results["performance_assessment"] = self._generate_assessment(results["test_results"])
            
        except Exception as e:
            results["test_end_time"] = datetime.now().isoformat()
            results["test_status"] = "failed"
            results["error"] = str(e)
            logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def _generate_assessment(self, test_results: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½è¯„ä¼°æŠ¥å‘Š"""
        assessment = {
            "overall_score": 0,
            "performance_grade": "æœªçŸ¥",
            "key_metrics": {},
            "recommendations": []
        }
        
        try:
            # åŸºç¡€è´Ÿè½½æµ‹è¯•è¯„ä¼°
            basic_load = test_results.get("basic_load_test", {})
            success_rate = float(basic_load.get("summary", {}).get("success_rate", "0%").replace('%', ''))
            avg_response_time = basic_load.get("response_time_stats", {}).get("average", 999)
            
            # è®¡ç®—åˆ†æ•°
            score = 0
            if success_rate >= 99:
                score += 30
            elif success_rate >= 95:
                score += 20
            elif success_rate >= 90:
                score += 10
            
            if avg_response_time <= 1.0:
                score += 30
            elif avg_response_time <= 3.0:
                score += 20
            elif avg_response_time <= 5.0:
                score += 10
            
            # å‹åŠ›æµ‹è¯•è¯„ä¼°
            stress_test = test_results.get("stress_test", {})
            max_users = stress_test.get("performance_bottleneck", {}).get("max_stable_users", 0)
            
            if max_users >= 40:
                score += 30
            elif max_users >= 20:
                score += 20
            elif max_users >= 10:
                score += 10
            
            # ç³»ç»Ÿèµ„æºè¯„ä¼°
            cpu_avg = basic_load.get("system_resource_stats", {}).get("cpu_usage", {}).get("average", 100)
            memory_avg = basic_load.get("system_resource_stats", {}).get("memory_usage", {}).get("average", 100)
            
            if cpu_avg <= 60 and memory_avg <= 70:
                score += 10
            elif cpu_avg <= 80 and memory_avg <= 85:
                score += 5
            
            assessment["overall_score"] = min(score, 100)
            
            # æ€§èƒ½ç­‰çº§
            if score >= 90:
                assessment["performance_grade"] = "ä¼˜ç§€"
            elif score >= 70:
                assessment["performance_grade"] = "è‰¯å¥½"
            elif score >= 50:
                assessment["performance_grade"] = "ä¸€èˆ¬"
            else:
                assessment["performance_grade"] = "éœ€è¦ä¼˜åŒ–"
            
            # å…³é”®æŒ‡æ ‡
            assessment["key_metrics"] = {
                "success_rate": f"{success_rate}%",
                "average_response_time": f"{avg_response_time:.2f}s",
                "max_concurrent_users": max_users,
                "cpu_usage_avg": f"{cpu_avg:.1f}%",
                "memory_usage_avg": f"{memory_avg:.1f}%"
            }
            
            # ä¼˜åŒ–å»ºè®®
            if success_rate < 95:
                assessment["recommendations"].append("æé«˜ç³»ç»Ÿç¨³å®šæ€§ï¼Œå‡å°‘é”™è¯¯ç‡")
            if avg_response_time > 3:
                assessment["recommendations"].append("ä¼˜åŒ–å“åº”æ—¶é—´ï¼Œè€ƒè™‘ç¼“å­˜æˆ–ç®—æ³•ä¼˜åŒ–")
            if max_users < 20:
                assessment["recommendations"].append("æå‡ç³»ç»Ÿå¹¶å‘å¤„ç†èƒ½åŠ›")
            if cpu_avg > 80:
                assessment["recommendations"].append("ä¼˜åŒ–CPUä½¿ç”¨ï¼Œè€ƒè™‘å¼‚æ­¥å¤„ç†")
            if memory_avg > 85:
                assessment["recommendations"].append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼")
            
            if not assessment["recommendations"]:
                assessment["recommendations"].append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")
        
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")
        
        return assessment

# æ‰§è¡Œæ€§èƒ½æµ‹è¯•
async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ CloudCoder æ€§èƒ½æµ‹è¯•å¼€å§‹...")
    print("=" * 60)
    
    tester = CloudCoderPerformanceTester()
    results = await tester.run_full_performance_suite()
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    with open("performance_test_report.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ‘˜è¦æŠ¥å‘Š
    print("\nğŸ“Š æ€§èƒ½æµ‹è¯•æ‘˜è¦æŠ¥å‘Š")
    print("=" * 60)
    
    if results.get("test_status") == "completed":
        assessment = results.get("performance_assessment", {})
        print(f"ğŸ¯ ç»¼åˆè¯„åˆ†: {assessment.get('overall_score', 0)}/100")
        print(f"ğŸ“ˆ æ€§èƒ½ç­‰çº§: {assessment.get('performance_grade', 'æœªçŸ¥')}")
        
        key_metrics = assessment.get("key_metrics", {})
        print(f"\nğŸ“‹ å…³é”®æŒ‡æ ‡:")
        for metric, value in key_metrics.items():
            print(f"   â€¢ {metric}: {value}")
        
        recommendations = assessment.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
    else:
        print(f"âŒ æµ‹è¯•çŠ¶æ€: {results.get('test_status')}")
        if results.get("error"):
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {results.get('error')}")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: performance_test_report.json")
    print("=" * 60)
    print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")